# -*- coding: utf-8 -*-
"""Membuat Model NLP dengan TensorFlow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rl8BJ6acDelUZitCz85F8VpGQn12r7Mp

# Download Data
"""

! pip install -q kaggle

from google.colab import files

files.upload()

! mkdir ~/.kaggle

! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets list

! kaggle datasets download -d goelyash/disney-hotstar-tv-and-movie-catalog

! unzip disney-hotstar-tv-and-movie-catalog.zip -d .

! pip install squarify

"""# Data Understanding"""

# Library
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
import squarify

df = pd.read_csv('hotstar.csv')
df.head()

df.info()

df.isnull().sum()

df.shape
print(2117/6228*100,4128/6228*100)

sns.boxplot(x=df['running_time'])

q1 = df['running_time'].quantile(0.25)
q3 = df['running_time'].quantile(0.75)
iqr = q3 - q1

print(iqr)

d = df[(df['running_time'] > q1 - 1.5 * iqr) & (df['running_time'] < q3 + 1.5 * iqr)]
#taking the mean 
mean=d['running_time'].mean()
print(mean)

d=df.copy()
d['running_time'].fillna(value=mean, inplace=True)

d["running_time"].isnull().sum()

d2=d.copy()
d2['running_time'].fillna(df['running_time'].mean())

sns.set(rc={'figure.figsize':(11.7,8.27)})
sns.kdeplot(x="running_time", data=d)
sns.kdeplot(data=d2,x='running_time')
plt.legend(labels=["Filled removing outliers","Filled without removing outliers"]);

df['running_time'].fillna(mean, inplace=True)

#filling the null values with mean
df['episodes'].fillna(df['episodes'].median(), inplace=True)
df['seasons'].fillna(df['seasons'].median(), inplace=True)

df.isnull().sum()

df["seasons"].unique()

df["type"].unique()

df["genre"].unique()

df["age_rating"].unique()

df.shape

"""## EDA"""

#count the  different counts of genre
df['genre'].value_counts()

#graph for showing the different instances of years
data=df['genre'].value_counts().sort_values(ascending=False)[:10]
sns.barplot(x=data.index, y=data)

#count the unique years
print("The total number of unique years in the dataset",df['year'].nunique())
#count the  different counts of years
df['year'].value_counts()

#graph showing the increase in the number of titles per year
data=df['year'].value_counts().sort_values(ascending=False)
sns.lineplot(x=data.index, y=data)

#graph for showcasing the different number of titles per year
data=df['year'].value_counts().sort_values(ascending=False)[:10]
sns.barplot(x=data.index, y=data)

#unique count of age ratings
print("The total number of unique age ratings in the dataset",df['age_rating'].nunique())
#count the  different counts of age ratings
df['age_rating'].value_counts()

#graph for the top 7 age ratings
data=df['age_rating'].value_counts().sort_values(ascending=False)[:7]
sns.barplot(x=data.index, y=data);

#avg running time
print("The average run time of the episode is",round(df['running_time'].mean(),2))
#avg episodes
print("The average number of episode per season is",round(df['episodes'].median(),2))
#avg seasons
print("The average number of seasons per title is",round(df['seasons'].median(),2))

#the different types
df['type'].value_counts()
#pie chart 
plt.pie(df['type'].value_counts(), labels=df['type'].value_counts().index, autopct='%1.1f%%');

#group by year and get only values of type 'movie'
df_movie=df[df['type']=='movie']
df_series=df[df['type']=='tv']
data1=df_movie.groupby(df['year'])['type'].count()
data2=df_series.groupby(df['year'])['type'].count()
#graph for showing the yearly increase in the movies each year
# sns.lineplot(x=data.index, y=data)
#draw a histogram using data

#line graphs
plt.plot(data1.index, data1.values, color='red', label='Movies')
plt.plot(data2.index, data2.values, color='blue', label='Series')
plt.legend(loc='upper left')
plt.xlabel('Year')
plt.ylabel('Number of titles')
plt.title('Yearly increase in the number of movies and series')
plt.show()

#top 10 genres in movies
data=df_movie['genre'].value_counts().sort_values(ascending=False)[:10]
#draw treemap
plt.figure(figsize=(10,10))
plt.title('Top 10 genres in movies')
squarify.plot(data,label=data.index, alpha=.8 )
plt.show()

#draw a pie chart for the top 10 genres in movies
plt.pie(data.values, labels=data.index, autopct='%1.1f%%');
plt.show()

#top 10 genres in series
data=df_series['genre'].value_counts().sort_values(ascending=False)[:10]
#draw treemap
plt.figure(figsize=(20,20))
plt.title('Top 10 genres in series')
squarify.plot(data,label=data.index, alpha=.8 )
plt.show()

#age rating of movies and tv
data=df_movie['age_rating'].value_counts().sort_values(ascending=False)[:10]
#draw pie chart
plt.pie(data.values, labels=data.index, autopct='%1.1f%%');
plt.show()

#age rating of tv
data=df_series['age_rating'].value_counts().sort_values(ascending=False)[:10]
#draw pie chart
plt.pie(data.values, labels=data.index, autopct='%1.1f%%');
plt.show()

#the biggest runtime for a movie
print("The biggest runtime for a movie is",round(df_movie['running_time'].max(),2))
#the biggest runtime for a series
print("The biggest runtime for a series is",round(df_series['running_time'].max(),2))

"""# Data Preparation"""

# Create a new dataframe
category = pd.get_dummies(df.type)
df_new = pd.concat([df, category], axis=1)
df_new = df_new.drop(columns='type')
df_new

# Convert values from dataframe into numpy array data type
description = df_new['description'].values
label = df_new[['tv', 'movie',]].values

# Data for training and testing
description_latih, description_test, label_latih, label_test = train_test_split(description, label, test_size=0.2)

# Tokenizer Function
tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(description_latih)
sequences = tokenizer.texts_to_sequences(description_latih)
sekuens_latih = pad_sequences(
    sequences,
    maxlen=None,
    dtype='int32',
    padding='pre',
    truncating='pre',
    value=0.0)
sekuens_test = tokenizer.texts_to_sequences(description_test)
padded_latih = pad_sequences(sekuens_latih)
padded_test = pad_sequences(sekuens_test)

"""# Modelling"""

# Embedding 
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(2, activation='softmax')
])
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

# Use of Callbacks
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.99):
      print("\nAkurasi telah mencapai >99%!")
      self.model.stop_training = True
callbacks = myCallback()

# Calling the fit() function
num_epochs = 50
history = model.fit(padded_latih, label_latih, epochs=num_epochs, callbacks=[callbacks],
                    validation_data=(padded_test, label_test), verbose=2)

# Retrieve Accuracy Value 
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
# Taking Loss Value 
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

# Plot Accruracy
plt.plot(epochs, acc, 'r', label='Train accuracy')
plt.plot(epochs, val_acc, 'g', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend(loc=0)
plt.figure()
plt.show()

# Plot Loss
plt.plot(epochs, loss, 'r', label='Train loss')
plt.plot(epochs, val_loss, 'g', label='Validation loss')
plt.title('Training and validation loss')
plt.legend(loc=0)
plt.figure()
plt.show()